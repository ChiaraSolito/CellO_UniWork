{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiaraSolito/CellO_UniWork/blob/main/exercises1_lab03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sASTP-TW4yCr"
      },
      "source": [
        "# Lab 03 - 1 #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lqZjtX14yCt"
      },
      "source": [
        "## Pedestrian Detection using Faster R-CNN ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByFnUWe84yCt"
      },
      "source": [
        "In this exercise we are going to explore a classic usage of a Faster RCNN model for pedestrian detection, using the COCO format.\n",
        "\n",
        "We are going to:\n",
        "* Download pycocotools to handle image datasets that use the COCO format\n",
        "* Define a dataset using the COCO format\n",
        "* Download a pre-trained Faster RCNN model\n",
        "* Edit the model to fine-tune it on our dataset\n",
        "* Train the model on our dataset\n",
        "\n",
        "In order to download the Penn-Fudan Database for Pedestrian Detection and Segmentation, run the following two commands:\n",
        "```\n",
        "!mkdir data\n",
        "!cd data && wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip && unzip -o PennFudanPed.zip &> /dev/null\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ2GRv8d3536",
        "outputId": "172bec2d-b46e-42d0-ae5f-fedc2d70839d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2023-04-03 16:41:23--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53723336 (51M) [application/zip]\n",
            "Saving to: ‘PennFudanPed.zip.2’\n",
            "\n",
            "PennFudanPed.zip.2  100%[===================>]  51.23M  59.4MB/s    in 0.9s    \n",
            "\n",
            "2023-04-03 16:41:24 (59.4 MB/s) - ‘PennFudanPed.zip.2’ saved [53723336/53723336]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!cd data && wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip && unzip -o PennFudanPed.zip &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "D4OPT-7y4yCt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import cv2\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import contextlib\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "# from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from pycocotools.coco import COCO\n",
        "from matplotlib import pyplot as plt\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools import mask as coco_mask\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "\n",
        "# Seed.\n",
        "np.random.seed(66)\n",
        "torch.manual_seed(66)\n",
        "\n",
        "# Hyperparameters.\n",
        "EPOCH = 5\n",
        "BATCH_SIZE = 1\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faXl92gD4yCu"
      },
      "source": [
        "### Define the dataset ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aCRA_EVe4yCu"
      },
      "outputs": [],
      "source": [
        "class PennFudanDataset(Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # Load all image files, sorting them to ensure that they are aligned.\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load images and masks.\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # Note that we haven't converted the mask to RGB, because each color\n",
        "        # corresponds to a different instance with 0 being background.\n",
        "        mask = Image.open(mask_path)\n",
        "        # Convert the PIL Image into a numpy array.\n",
        "        mask = np.array(mask)\n",
        "        # Instances are encoded as different colors.\n",
        "        obj_ids = np.unique(mask)\n",
        "        # The first id is the background, so remove it.\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # Split the color-encoded mask into a set of binary masks.\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # Get bounding box coordinates for each mask.\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        # Convert everything into a torch.Tensor.\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # There is only one class.\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # Suppose all instances are not crowd.\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        else:\n",
        "            img = np.asarray(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "g5BKp8A64yCv"
      },
      "outputs": [],
      "source": [
        "def show_dataset(sample_data_loader):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_samples = 3\n",
        "    plot_cols = 3\n",
        "    _plot_idx = 0\n",
        "    for i, (image, metadata) in enumerate(sample_data_loader):\n",
        "        if i >= plot_samples:\n",
        "            break\n",
        "\n",
        "        _plot_idx += 1\n",
        "        plt.subplot(plot_samples, plot_cols, _plot_idx)\n",
        "        image = image.squeeze().numpy()\n",
        "        print(image.shape)\n",
        "        plt.imshow(image)\n",
        "\n",
        "        _plot_idx += 1\n",
        "        plt.subplot(plot_samples, plot_cols, _plot_idx)\n",
        "        boxes = metadata[\"boxes\"]  # 1, N, 4.\n",
        "        if len(boxes.shape) == 3:\n",
        "            boxes = boxes.squeeze(0)\n",
        "        image_with_bbox = image.copy()\n",
        "        for bbox in boxes:\n",
        "            xmin, ymin, xmax, ymax = bbox\n",
        "            xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
        "            pt1 = (xmin, ymin)\n",
        "            pt2 = (xmax, ymax)\n",
        "            cv2.rectangle(image_with_bbox, pt1, pt2, (0, 215, 0), 10)\n",
        "        plt.imshow(image_with_bbox)\n",
        "\n",
        "        _plot_idx += 1\n",
        "        # plt.subplot(plot_samples, plot_cols, _plot_idx)\n",
        "        # masks = metadata[\"masks\"].squeeze(0)\n",
        "        # s = len(masks.shape)\n",
        "        # if s == 3:\n",
        "        #     masks = torch.sum(masks, dim=0)\n",
        "\n",
        "        # m = masks.numpy().astype(np.uint8)\n",
        "        # m[m > 0] = 255\n",
        "        # plt.imshow(np.dstack((m, m, m)))\n",
        "\n",
        "# Perform data visualization.\n",
        "# TODO.\n",
        "\n",
        "# You may want to use a DataLoader, but it's not mandatory.\n",
        "image_data = PennFudanDataset(\"./data/PennFudanPed/\", get_transform(True))\n",
        "\n",
        "image_loader = torch.utils.data.DataLoader(image_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# For each sample show:\n",
        "# 1) The RGB image\n",
        "# 2) The Bounding Boxes over the RGB frame\n",
        "#    - HINT: use cv2.rectangle() to draw a rect, or matplotlib.patches.Rectangle()\n",
        "#show_dataset(image_loader)\n",
        "# TODO."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6kBsaSO4yCw"
      },
      "source": [
        "### Modify the DNN ###\n",
        "\n",
        "We are going to change the classification head with one having the correct dimensions (features and classes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bhhHb__74yCw"
      },
      "outputs": [],
      "source": [
        "# Load a model pre-trained on COCO.\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
        "\n",
        "\n",
        "# Replace the classifier with a new one, that has num_classes which is user-defined.\n",
        "# 1 class (person) + background.\n",
        "num_classes = 2\n",
        "\n",
        "# Get number of input features for the classifier.\n",
        "in_features = model.roi_heads.box_predictor.bbox_pred.in_features\n",
        "\n",
        "# Replace the pre-trained head with a new one.\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "#print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NYtjQZdv4yCw"
      },
      "outputs": [],
      "source": [
        "# Transformations.\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # If you want, many different Data Augmentation\n",
        "        # options are available for training phase.\n",
        "\n",
        "        # transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "        # RandomHorizontalFlip is one of them, but you may want to see also:\n",
        "        #   ColorJitter\n",
        "        #   RandomGrayscale\n",
        "        #   RandomAffine\n",
        "        pass\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "# Utils for train/test DataLoaders.\n",
        "def collate_fn(batch): \n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "oFCnT5Lc4yCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b626c84-066c-44ea-83c9-b61b15e0cf2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PennFudanDataset - train: 127\n",
            "PennFudanDataset - test: 43\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dataset.\n",
        "dataset = PennFudanDataset(\"./data/PennFudanPed/\", get_transform(True))\n",
        "\n",
        "# Train and test split.\n",
        "train_data, test_data = train_test_split(dataset)\n",
        "\n",
        "# DataLoaders.\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "print(f\"PennFudanDataset - train: {len(train_loader) * BATCH_SIZE}\")\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "print(f\"PennFudanDataset - test: {len(test_loader) * BATCH_SIZE}\")\n",
        "\n",
        "# Move model to the right device.\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Define the optimizer and a learning rate scheduler.\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params, \n",
        "    lr=0.005, \n",
        "    momentum=0.9, \n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3,\n",
        "                                               gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PennFudanDataset(\"./data/PennFudanPed/\", get_transform(True))\n",
        "for img,label in dataset:\n",
        "  print(label['boxes'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn0I3CbvCtwz",
        "outputId": "8cf979f6-dcdd-4fe9-f9b0-78c34d4f8ea2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[159., 181., 301., 430.],\n",
            "        [419., 170., 534., 485.]])\n",
            "tensor([[ 67.,  92., 190., 379.]])\n",
            "tensor([[292., 134., 446., 420.]])\n",
            "tensor([[167.,  59., 323., 337.],\n",
            "        [  8.,  60.,  47., 179.]])\n",
            "tensor([[187.,  58., 319., 335.],\n",
            "        [  1.,  52.,  39., 157.]])\n",
            "tensor([[207., 107., 345., 384.],\n",
            "        [  1., 107.,  86., 383.]])\n",
            "tensor([[111.,  68., 217., 345.],\n",
            "        [377.,  75., 528., 376.],\n",
            "        [316., 107., 346., 191.]])\n",
            "tensor([[227., 157., 369., 435.],\n",
            "        [ 38., 178., 114., 362.]])\n",
            "tensor([[305., 137., 452., 429.],\n",
            "        [156., 123., 297., 397.]])\n",
            "tensor([[280.,  89., 400., 373.]])\n",
            "tensor([[277., 111., 437., 393.]])\n",
            "tensor([[158.,  70., 294., 360.],\n",
            "        [327.,  57., 438., 326.]])\n",
            "tensor([[388., 192., 553., 475.]])\n",
            "tensor([[233.,  85., 404., 366.]])\n",
            "tensor([[ 18.,  42., 173., 326.]])\n",
            "tensor([[ 79.,  85., 204., 382.],\n",
            "        [278.,  93., 399., 360.],\n",
            "        [410., 100., 494., 377.]])\n",
            "tensor([[114.,  47., 243., 331.]])\n",
            "tensor([[ 19.,  18., 125., 303.]])\n",
            "tensor([[  6., 134., 141., 388.],\n",
            "        [193., 122., 338., 420.]])\n",
            "tensor([[338.,  98., 507., 380.]])\n",
            "tensor([[323.,  75., 469., 366.],\n",
            "        [233.,  92., 267., 166.],\n",
            "        [ 46.,  94.,  68., 159.]])\n",
            "tensor([[111., 168., 208., 448.],\n",
            "        [396., 170., 513., 454.]])\n",
            "tensor([[201.,  83., 340., 365.]])\n",
            "tensor([[102.,  83., 269., 369.]])\n",
            "tensor([[225.,  68., 395., 353.],\n",
            "        [ 43.,  86.,  93., 255.],\n",
            "        [127.,  73., 188., 261.],\n",
            "        [179.,  52., 227., 254.],\n",
            "        [212.,  72., 272., 261.],\n",
            "        [313.,  61., 382., 265.]])\n",
            "tensor([[ 43.,  77., 149., 388.],\n",
            "        [184.,  88., 278., 372.]])\n",
            "tensor([[103.,  44., 252., 327.]])\n",
            "tensor([[  6.,  15., 148., 302.],\n",
            "        [185.,  17., 315., 320.]])\n",
            "tensor([[245.,  71., 367., 350.]])\n",
            "tensor([[ 58.,  84., 207., 374.]])\n",
            "tensor([[297., 121., 443., 407.]])\n",
            "tensor([[455., 161., 572., 452.]])\n",
            "tensor([[164.,  52., 303., 345.]])\n",
            "tensor([[110.,  36., 253., 329.]])\n",
            "tensor([[ 63.,  38., 194., 330.]])\n",
            "tensor([[130.,  97., 278., 393.],\n",
            "        [801., 119., 936., 380.],\n",
            "        [243., 132., 329., 362.],\n",
            "        [725., 144., 788., 344.]])\n",
            "tensor([[258.,  62., 414., 348.],\n",
            "        [189.,  97., 208., 169.]])\n",
            "tensor([[218.,  43., 399., 330.]])\n",
            "tensor([[136., 136., 198., 310.],\n",
            "        [230., 129., 330., 409.]])\n",
            "tensor([[200., 156., 278., 436.],\n",
            "        [278., 171., 376., 429.]])\n",
            "tensor([[350., 160., 438., 436.],\n",
            "        [283., 180., 356., 438.],\n",
            "        [ 41., 185., 109., 387.]])\n",
            "tensor([[158., 185., 249., 469.],\n",
            "        [287., 155., 391., 448.]])\n",
            "tensor([[ 63., 138., 146., 425.],\n",
            "        [155., 138., 247., 434.],\n",
            "        [260., 127., 357., 430.]])\n",
            "tensor([[245., 169., 339., 461.],\n",
            "        [336., 174., 425., 463.],\n",
            "        [ 36., 128., 180., 491.],\n",
            "        [435., 156., 500., 401.]])\n",
            "tensor([[197., 197., 293., 480.],\n",
            "        [318., 200., 402., 498.],\n",
            "        [394., 211., 468., 494.]])\n",
            "tensor([[177., 122., 270., 409.],\n",
            "        [298., 113., 369., 334.],\n",
            "        [445., 103., 506., 314.],\n",
            "        [520., 118., 564., 257.]])\n",
            "tensor([[271., 197., 370., 481.],\n",
            "        [ 30., 194.,  89., 426.],\n",
            "        [383., 184., 429., 297.]])\n",
            "tensor([[  5., 241.,  83., 534.],\n",
            "        [ 78., 230., 143., 454.],\n",
            "        [157., 218., 250., 506.],\n",
            "        [459., 262., 579., 543.]])\n",
            "tensor([[126., 122., 212., 411.],\n",
            "        [213., 105., 316., 391.],\n",
            "        [314., 106., 434., 386.]])\n",
            "tensor([[ 32., 202., 134., 483.],\n",
            "        [290., 204., 368., 411.]])\n",
            "tensor([[125., 190., 217., 475.]])\n",
            "tensor([[205., 188., 307., 491.]])\n",
            "tensor([[ 93., 168., 194., 431.],\n",
            "        [388., 149., 498., 452.]])\n",
            "tensor([[ 96., 134., 181., 417.],\n",
            "        [286., 113., 357., 331.],\n",
            "        [363., 120., 436., 328.]])\n",
            "tensor([[147., 177., 276., 529.]])\n",
            "tensor([[438., 246., 536., 521.],\n",
            "        [199., 258., 269., 528.],\n",
            "        [251., 255., 373., 527.]])\n",
            "tensor([[ 21.,  90., 142., 405.],\n",
            "        [130.,  92., 204., 357.],\n",
            "        [236.,  82., 311., 355.],\n",
            "        [342., 116., 387., 266.],\n",
            "        [385., 115., 426., 249.]])\n",
            "tensor([[144.,  91., 260., 462.],\n",
            "        [ 42., 122.,  63., 210.],\n",
            "        [288., 101., 340., 279.],\n",
            "        [352.,  96., 405., 275.],\n",
            "        [426., 125., 445., 181.],\n",
            "        [461., 125., 473., 170.],\n",
            "        [472., 132., 488., 182.],\n",
            "        [491., 120., 516., 201.]])\n",
            "tensor([[ 34.,  53., 134., 329.],\n",
            "        [158.,  35., 251., 328.],\n",
            "        [269.,  44., 379., 327.]])\n",
            "tensor([[187., 140., 290., 426.],\n",
            "        [292., 129., 383., 429.],\n",
            "        [394., 148., 497., 426.]])\n",
            "tensor([[164.,  69., 265., 352.],\n",
            "        [342.,  85., 428., 380.],\n",
            "        [405.,  84., 530., 386.]])\n",
            "tensor([[174.,  24., 254., 320.],\n",
            "        [290.,  33., 376., 329.]])\n",
            "tensor([[ 28.,  37., 113., 331.],\n",
            "        [155.,  41., 261., 328.],\n",
            "        [311.,  53., 329.,  98.],\n",
            "        [296.,  53., 302.,  72.],\n",
            "        [304.,  52., 309.,  72.]])\n",
            "tensor([[140.,  97., 239., 386.],\n",
            "        [316., 100., 399., 394.],\n",
            "        [399., 111., 516., 406.]])\n",
            "tensor([[ 28.,  68., 146., 381.],\n",
            "        [146.,  83., 241., 384.],\n",
            "        [233.,  69., 310., 392.],\n",
            "        [341.,  87., 429., 350.],\n",
            "        [ 12.,  57.,  63., 182.]])\n",
            "tensor([[247.,  49., 328., 350.]])\n",
            "tensor([[ 61.,  78., 183., 374.]])\n",
            "tensor([[248.,  65., 317., 360.]])\n",
            "tensor([[214.,  69., 314., 362.]])\n",
            "tensor([[287.,  69., 369., 363.]])\n",
            "tensor([[120.,  43., 245., 329.],\n",
            "        [255.,  36., 338., 347.],\n",
            "        [225.,  51., 254., 118.]])\n",
            "tensor([[ 47.,  39., 138., 331.]])\n",
            "tensor([[ 55.,  65., 158., 359.],\n",
            "        [192.,  58., 288., 351.]])\n",
            "tensor([[ 47.,  50., 125., 350.],\n",
            "        [177.,  60., 255., 344.]])\n",
            "tensor([[ 82.,  65., 196., 352.],\n",
            "        [264.,  74., 354., 337.],\n",
            "        [402.,  37., 500., 347.],\n",
            "        [513.,  64., 609., 317.],\n",
            "        [205.,  24., 265., 195.]])\n",
            "tensor([[  8.,  83.,  96., 303.],\n",
            "        [ 82.,  44., 164., 297.],\n",
            "        [267.,  83., 369., 297.],\n",
            "        [362.,  32., 515., 334.],\n",
            "        [575.,  74., 712., 361.],\n",
            "        [516.,  91., 607., 299.]])\n",
            "tensor([[  7.,  32., 173., 364.],\n",
            "        [451.,  63., 530., 352.],\n",
            "        [530.,  90., 660., 380.]])\n",
            "tensor([[ 66.,  60., 169., 409.],\n",
            "        [154.,  90., 252., 384.],\n",
            "        [227.,  93., 324., 390.],\n",
            "        [416., 100., 515., 390.],\n",
            "        [594., 111., 733., 396.]])\n",
            "tensor([[ 67., 129., 158., 404.],\n",
            "        [164., 153., 251., 409.],\n",
            "        [262., 143., 414., 438.],\n",
            "        [539., 168., 665., 439.],\n",
            "        [601., 155., 700., 395.]])\n",
            "tensor([[110.,  71., 198., 411.],\n",
            "        [230.,  95., 328., 387.],\n",
            "        [442., 107., 607., 434.],\n",
            "        [  2., 101.,  68., 396.]])\n",
            "tensor([[ 41.,  59., 164., 349.],\n",
            "        [126.,  82., 200., 358.],\n",
            "        [186.,  75., 308., 368.],\n",
            "        [298.,  75., 380., 399.],\n",
            "        [425.,  67., 526., 342.]])\n",
            "tensor([[ 46.,  70., 176., 365.],\n",
            "        [198.,  76., 304., 328.],\n",
            "        [255.,  73., 393., 344.],\n",
            "        [342.,  52., 420., 277.]])\n",
            "tensor([[ 46.,  84., 146., 291.],\n",
            "        [162.,  86., 239., 347.],\n",
            "        [224.,  85., 299., 306.],\n",
            "        [315.,  80., 390., 368.],\n",
            "        [374.,  99., 470., 350.],\n",
            "        [469.,  93., 558., 394.],\n",
            "        [532.,  89., 604., 375.]])\n",
            "tensor([[ 17.,  58., 102., 339.],\n",
            "        [185.,  55., 261., 336.],\n",
            "        [308., 140., 435., 492.],\n",
            "        [474.,  75., 617., 450.],\n",
            "        [391.,  57., 484., 420.],\n",
            "        [562.,  35., 680., 393.]])\n",
            "tensor([[ 91.,  61., 235., 343.],\n",
            "        [241.,  51., 300., 354.]])\n",
            "tensor([[113., 121., 211., 402.]])\n",
            "tensor([[ 97.,  78., 209., 358.],\n",
            "        [258.,  95., 363., 358.],\n",
            "        [383.,  80., 497., 354.],\n",
            "        [214.,  90., 256., 192.]])\n",
            "tensor([[126.,  39., 205., 228.],\n",
            "        [208.,  68., 311., 336.],\n",
            "        [353.,  59., 447., 336.],\n",
            "        [285.,  40., 352., 224.]])\n",
            "tensor([[122.,  97., 279., 413.],\n",
            "        [285.,  90., 447., 377.],\n",
            "        [748.,  93., 833., 381.]])\n",
            "tensor([[ 85.,  50., 145., 361.],\n",
            "        [235.,  92., 328., 381.],\n",
            "        [467.,  65., 578., 366.]])\n",
            "tensor([[114.,  49., 203., 386.],\n",
            "        [258.,  48., 397., 335.]])\n",
            "tensor([[ 64.,  31., 144., 326.],\n",
            "        [180.,  68., 261., 307.]])\n",
            "tensor([[ 16.,  77., 105., 355.],\n",
            "        [159.,  69., 248., 360.],\n",
            "        [314.,  59., 388., 363.],\n",
            "        [367.,  46., 472., 369.],\n",
            "        [463.,  74., 538., 361.],\n",
            "        [524.,  79., 635., 374.],\n",
            "        [599.,  77., 703., 374.]])\n",
            "tensor([[141., 120., 263., 378.],\n",
            "        [386.,  75., 515., 364.],\n",
            "        [444.,  76., 606., 368.]])\n",
            "tensor([[ 67., 117., 217., 406.],\n",
            "        [463.,  80., 585., 381.],\n",
            "        [558., 110., 680., 380.]])\n",
            "tensor([[ 29., 117., 166., 412.],\n",
            "        [119., 148., 256., 412.],\n",
            "        [215., 151., 353., 415.],\n",
            "        [704., 128., 838., 389.],\n",
            "        [344., 161., 423., 341.]])\n",
            "tensor([[106.,  59., 274., 362.]])\n",
            "tensor([[ 70.,  57., 169., 349.]])\n",
            "tensor([[ 71.,  33., 215., 309.],\n",
            "        [188., 150., 275., 303.],\n",
            "        [389.,   1., 420.,  70.]])\n",
            "tensor([[302.,  93., 446., 381.],\n",
            "        [199., 119., 290., 372.]])\n",
            "tensor([[125.,  92., 211., 362.],\n",
            "        [203.,  80., 321., 366.],\n",
            "        [346.,  88., 462., 341.]])\n",
            "tensor([[210.,  49., 324., 333.]])\n",
            "tensor([[113.,  64., 213., 369.],\n",
            "        [273.,  97., 338., 390.],\n",
            "        [362., 101., 465., 387.],\n",
            "        [508., 102., 584., 391.]])\n",
            "tensor([[138.,  36., 252., 338.],\n",
            "        [  2.,  36.,  94., 341.],\n",
            "        [315.,  42., 447., 338.]])\n",
            "tensor([[243.,  41., 389., 328.]])\n",
            "tensor([[ 97.,  49., 202., 328.]])\n",
            "tensor([[ 31.,  36., 125., 369.],\n",
            "        [109.,  49., 213., 375.],\n",
            "        [391.,  69., 518., 367.],\n",
            "        [273., 148., 388., 332.]])\n",
            "tensor([[107.,  57., 288., 390.],\n",
            "        [301.,  62., 404., 343.],\n",
            "        [368.,  66., 492., 352.],\n",
            "        [180.,  62., 277., 373.]])\n",
            "tensor([[168.,  29., 274., 330.],\n",
            "        [366.,  53., 540., 332.],\n",
            "        [120.,  32., 207., 321.]])\n",
            "tensor([[ 55.,  61., 163., 329.],\n",
            "        [513.,  33., 635., 327.]])\n",
            "tensor([[228.,  23., 327., 306.]])\n",
            "tensor([[231.,  38., 367., 315.]])\n",
            "tensor([[325.,  72., 410., 359.]])\n",
            "tensor([[202.,  41., 317., 329.]])\n",
            "tensor([[352.,  98., 466., 379.]])\n",
            "tensor([[272.,  36., 378., 311.],\n",
            "        [402.,  35., 471., 309.]])\n",
            "tensor([[188.,  46., 296., 333.],\n",
            "        [331.,  62., 433., 327.],\n",
            "        [141.,  77., 198., 253.],\n",
            "        [281.,  77., 327., 252.],\n",
            "        [429.,  72., 455., 153.]])\n",
            "tensor([[ 70.,  35., 169., 328.],\n",
            "        [252.,  62., 335., 281.],\n",
            "        [331.,  53., 407., 275.],\n",
            "        [180.,  75., 224., 192.]])\n",
            "tensor([[ 69.,  63., 192., 364.],\n",
            "        [225.,  64., 320., 359.],\n",
            "        [323.,  66., 435., 346.],\n",
            "        [525.,  44., 638., 347.],\n",
            "        [424.,  66., 484., 216.],\n",
            "        [493.,  61., 547., 213.]])\n",
            "tensor([[ 20.,  57., 110., 322.],\n",
            "        [317.,  21., 419., 318.],\n",
            "        [420.,  61., 440., 118.]])\n",
            "tensor([[115.,  56., 252., 335.],\n",
            "        [245.,  32., 380., 319.],\n",
            "        [361.,  41., 426., 193.]])\n",
            "tensor([[ 26.,  32., 117., 325.],\n",
            "        [115.,  34., 197., 320.],\n",
            "        [262.,  20., 384., 368.]])\n",
            "tensor([[ 81.,  47., 174., 345.],\n",
            "        [174.,  44., 252., 336.],\n",
            "        [251.,  80., 306., 236.],\n",
            "        [309.,  76., 367., 239.]])\n",
            "tensor([[ 90.,  40., 195., 290.]])\n",
            "tensor([[ 37.,  51., 202., 325.],\n",
            "        [313.,  66., 480., 364.],\n",
            "        [458.,  33., 586., 372.],\n",
            "        [553.,  66., 627., 338.]])\n",
            "tensor([[ 76., 136., 174., 427.],\n",
            "        [168., 180., 293., 490.],\n",
            "        [402., 156., 493., 399.],\n",
            "        [523.,  82., 612., 392.],\n",
            "        [278., 178., 333., 313.]])\n",
            "tensor([[ 14.,  28., 123., 324.],\n",
            "        [168.,  26., 265., 328.]])\n",
            "tensor([[ 42.,  21., 127., 316.]])\n",
            "tensor([[ 95.,  50., 207., 340.],\n",
            "        [ 28.,   7., 121., 282.]])\n",
            "tensor([[174., 105., 270., 397.],\n",
            "        [139., 121., 192., 383.],\n",
            "        [256.,  97., 305., 385.]])\n",
            "tensor([[157.,  63., 260., 351.],\n",
            "        [197.,  17., 265., 293.]])\n",
            "tensor([[166.,  26., 244., 315.],\n",
            "        [227.,  30., 306., 290.],\n",
            "        [321.,  65., 451., 350.],\n",
            "        [294.,  21., 373., 273.]])\n",
            "tensor([[ 79.,  43., 176., 336.],\n",
            "        [380.,  35., 528., 322.]])\n",
            "tensor([[ 42.,  22., 141., 325.],\n",
            "        [188.,  20., 282., 229.],\n",
            "        [116.,  27., 190., 225.],\n",
            "        [272.,  80., 388., 326.],\n",
            "        [377.,  30., 475., 329.]])\n",
            "tensor([[ 33.,  16., 142., 306.],\n",
            "        [249.,  56., 274., 145.]])\n",
            "tensor([[ 85.,  14., 183., 300.],\n",
            "        [175.,  34., 287., 304.],\n",
            "        [  1.,  45.,  56., 222.],\n",
            "        [320.,  53., 372., 233.]])\n",
            "tensor([[ 87.,  49., 261., 338.]])\n",
            "tensor([[225.,  28., 333., 313.]])\n",
            "tensor([[ 23.,  20., 138., 303.]])\n",
            "tensor([[ 10.,  75., 127., 364.],\n",
            "        [264.,  32., 397., 311.],\n",
            "        [ 75.,  29., 188., 344.]])\n",
            "tensor([[ 36.,  12., 183., 297.]])\n",
            "tensor([[ 42.,  88., 152., 363.],\n",
            "        [188.,  55., 282., 348.],\n",
            "        [430.,  36., 567., 348.],\n",
            "        [383.,  46., 454., 222.]])\n",
            "tensor([[ 58.,  43., 148., 329.],\n",
            "        [168.,  39., 268., 309.]])\n",
            "tensor([[ 14.,  23., 121., 340.],\n",
            "        [184.,  13., 305., 311.]])\n",
            "tensor([[ 48.,  18., 170., 360.],\n",
            "        [291.,  43., 385., 304.],\n",
            "        [475.,  24., 566., 301.],\n",
            "        [600.,  12., 715., 305.],\n",
            "        [566.,  52., 611., 179.],\n",
            "        [708.,  52., 743., 173.]])\n",
            "tensor([[133.,  24., 253., 336.],\n",
            "        [279.,  51., 430., 329.]])\n",
            "tensor([[ 97.,  37., 190., 325.],\n",
            "        [181.,  45., 276., 339.],\n",
            "        [315.,  50., 412., 330.]])\n",
            "tensor([[259.,  30., 421., 316.],\n",
            "        [197.,  22., 282., 302.],\n",
            "        [116.,  47., 222., 294.]])\n",
            "tensor([[193.,  22., 334., 307.]])\n",
            "tensor([[186.,  28., 319., 318.]])\n",
            "tensor([[ 64.,  26., 180., 316.]])\n",
            "tensor([[215.,  58., 319., 354.]])\n",
            "tensor([[205.,  24., 323., 319.]])\n",
            "tensor([[ 71.,  46., 174., 338.],\n",
            "        [197.,  42., 313., 338.],\n",
            "        [319.,  43., 421., 331.]])\n",
            "tensor([[147.,  84., 238., 365.],\n",
            "        [232.,  53., 336., 359.],\n",
            "        [329.,  74., 454., 365.]])\n",
            "tensor([[ 79.,  21., 228., 311.]])\n",
            "tensor([[107.,  31., 248., 326.],\n",
            "        [ 37.,  21., 123., 298.]])\n",
            "tensor([[141.,  45., 309., 341.],\n",
            "        [273.,  20., 361., 333.],\n",
            "        [160.,  94., 207., 257.]])\n",
            "tensor([[ 88.,  26., 199., 301.],\n",
            "        [244.,  37., 367., 328.]])\n",
            "tensor([[  7.,  32., 108., 252.],\n",
            "        [207.,  48., 305., 317.],\n",
            "        [344.,  31., 456., 324.],\n",
            "        [ 72., 162., 179., 352.],\n",
            "        [112.,  64., 163., 196.]])\n",
            "tensor([[ 22.,  15., 147., 321.]])\n",
            "tensor([[ 72.,  17., 214., 311.]])\n",
            "tensor([[ 84.,  24., 164., 293.],\n",
            "        [139.,   5., 296., 308.]])\n",
            "tensor([[ 81.,  22., 211., 357.],\n",
            "        [ 42.,  38., 121., 345.],\n",
            "        [205.,  37., 365., 325.]])\n",
            "tensor([[211.,  25., 335., 320.]])\n",
            "tensor([[105.,  75., 310., 433.],\n",
            "        [287.,  93., 463., 389.]])\n",
            "tensor([[ 50.,   5., 221., 301.]])\n",
            "tensor([[121.,  26., 255., 321.]])\n",
            "tensor([[192.,  49., 299., 336.],\n",
            "        [319.,  54., 431., 334.]])\n",
            "tensor([[  5.,  37., 101., 323.],\n",
            "        [100.,  25., 205., 322.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "dIHHtFbf4yCw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708,
          "referenced_widgets": [
            "4ed4129e1da34b59a572026c82253b4f",
            "745c9a75d9b148f492f176604cc11c80",
            "990d7e70ed7a46b493719c27112c71f6",
            "8caea0806ba9410198b0877e7bb6f2d8",
            "3451705cfb9647a2b37ca51c94f1bc4f",
            "69dee04e330c4288886be1084aad55bd",
            "30c928d7a4b745d7895289f60a74bd80",
            "3e322cde81fd4ca4a9dd5a4f7952e238",
            "12908a6020214d0fbbd15c9419cc911b",
            "c51285e9e40547d58650bcf647a425fa",
            "36ca960bf5d6408787c7f646d33c0f45"
          ]
        },
        "outputId": "28e55609-f88c-4397-f2b7-4956f5681f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training on cpu [...]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/127 [00:00<?, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ed4129e1da34b59a572026c82253b4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-f0b7099f4335>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Update the learning rate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-f0b7099f4335>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mmodel_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmodel_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         torch._assert(\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ],
      "source": [
        "# Training step.\n",
        "print(f\"Start training on {DEVICE} [...]\")\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "\n",
        "    for i, (images, targets) in (tepoch := tqdm(enumerate(data_loader), unit=\"batch\", total=len(data_loader))):\n",
        "        tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        # Step 1: send the image to the required device.\n",
        "        # Images is a list of B images (where B = batch_size of the DataLoader).\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        # Step 2: send each target to the required device\n",
        "        # Targets is a dictionary of metadata. each (k,v) pair is a metadata\n",
        "        # required for training.\n",
        "        targets = {k: v.to(device) for k, v in targets.items()}\n",
        "        model_time = time.time()\n",
        "        loss_dict = model(images, targets)\n",
        "        model_time = time.time() - model_time\n",
        "        print(loss_dict)\n",
        "        break\n",
        "\n",
        "        # Step 3. backward on loss.\n",
        "        # Normally, you would obtain the loss from the model.forward()\n",
        "        # and then just call .bacward() on it.\n",
        "        # In this case, for each task, you have a different loss, due to\n",
        "        # different error metrics adopted by the tasks.\n",
        "        # One typical approach is to combine all the losses to one single loss,\n",
        "        # and then then backward that single loss.\n",
        "        # In this way you can adjust the weight of the different tasks,\n",
        "        # multiplying each loss for a hyperparemeter.\n",
        "        # E.G.:\n",
        "        #       final_loss = loss_1 + gamma*(alpha*loss_2 + beta*loss_3)\n",
        "        # In this case, we want to sum up all the losses.\n",
        "\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    train_epoch(model,optimizer,train_data,DEVICE,epoch)\n",
        "    break\n",
        "    # Update the learning rate.\n",
        "    lr_scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcF5k70l4yCx"
      },
      "outputs": [],
      "source": [
        "class CocoEvaluator:\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        assert isinstance(iou_types, (list, tuple))\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "            with contextlib.redirect_stdout(io.StringIO()):\n",
        "                coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = coco_evaluate(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(f\"IoU metric: {iou_type}\")\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        if iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        if iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        raise ValueError(f\"Unknown iou type {iou_type}\")\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0] for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"keypoints\": keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    import torch.distributed as dist\n",
        "\n",
        "    def is_dist_avail_and_initialized():\n",
        "        if not dist.is_available():\n",
        "            return False\n",
        "        if not dist.is_initialized():\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def get_world_size():\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return 1\n",
        "        return dist.get_world_size()\n",
        "\n",
        "    def all_gather(data):\n",
        "        world_size = get_world_size()\n",
        "        if world_size == 1:\n",
        "            return [data]\n",
        "        data_list = [None] * world_size\n",
        "        dist.all_gather_object(data_list, data)\n",
        "        return data_list\n",
        "\n",
        "    all_img_ids = all_gather(img_ids)\n",
        "    all_eval_imgs = all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # Keep only unique (and in sorted order) images.\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "def coco_evaluate(imgs):\n",
        "    with contextlib.redirect_stdout(io.StringIO()):\n",
        "        imgs.evaluate()\n",
        "    return imgs.params.imgIds, np.asarray(imgs.evalImgs).reshape(-1, len(imgs.params.areaRng), len(imgs.params.imgIds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E6eAx8A4yCx"
      },
      "outputs": [],
      "source": [
        "class FilterAndRemapCocoCategories:\n",
        "    def __init__(self, categories, remap=True):\n",
        "        self.categories = categories\n",
        "        self.remap = remap\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        anno = target[\"annotations\"]\n",
        "        anno = [obj for obj in anno if obj[\"category_id\"] in self.categories]\n",
        "        if not self.remap:\n",
        "            target[\"annotations\"] = anno\n",
        "            return image, target\n",
        "        anno = copy.deepcopy(anno)\n",
        "        for obj in anno:\n",
        "            obj[\"category_id\"] = self.categories.index(obj[\"category_id\"])\n",
        "        target[\"annotations\"] = anno\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask:\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if obj[\"iscrowd\"] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "        masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] for obj in anno])\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def _coco_remove_images_without_annotations(dataset, cat_list=None):\n",
        "    def _has_only_empty_bbox(anno):\n",
        "        return all(any(o <= 1 for o in obj[\"bbox\"][2:]) for obj in anno)\n",
        "\n",
        "    def _count_visible_keypoints(anno):\n",
        "        return sum(sum(1 for v in ann[\"keypoints\"][2::3] if v > 0) for ann in anno)\n",
        "\n",
        "    min_keypoints_per_image = 10\n",
        "\n",
        "    def _has_valid_annotation(anno):\n",
        "        # If it's empty, there is no annotation.\n",
        "        if len(anno) == 0:\n",
        "            return False\n",
        "        # If all boxes have close to zero area, there is no annotation.\n",
        "        if _has_only_empty_bbox(anno):\n",
        "            return False\n",
        "        # Keypoints task have a slight different critera for considering\n",
        "        # if an annotation is valid.\n",
        "        if \"keypoints\" not in anno[0]:\n",
        "            return True\n",
        "        # For keypoint detection tasks, only consider valid images those\n",
        "        # containing at least min_keypoints_per_image.\n",
        "        if _count_visible_keypoints(anno) >= min_keypoints_per_image:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    assert isinstance(dataset, torchvision.datasets.CocoDetection)\n",
        "    ids = []\n",
        "    for ds_idx, img_id in enumerate(dataset.ids):\n",
        "        ann_ids = dataset.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
        "        anno = dataset.coco.loadAnns(ann_ids)\n",
        "        if cat_list:\n",
        "            anno = [obj for obj in anno if obj[\"category_id\"] in cat_list]\n",
        "        if _has_valid_annotation(anno):\n",
        "            ids.append(ds_idx)\n",
        "\n",
        "    dataset = torch.utils.data.Subset(dataset, ids)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def convert_to_coco_api(ds):\n",
        "    coco_ds = COCO()\n",
        "    # Annotation IDs need to start at 1, not 0, see torchvision issue #1530.\n",
        "    ann_id = 1\n",
        "    dataset = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
        "    categories = set()\n",
        "    for img_idx in range(len(ds)):\n",
        "        # Find better way to get target.\n",
        "        # targets = ds.get_annotations(img_idx)\n",
        "        img, targets = ds[img_idx]\n",
        "        image_id = targets[\"image_id\"].item()\n",
        "        img_dict = {}\n",
        "        img_dict[\"id\"] = image_id\n",
        "        img_dict[\"height\"] = img.shape[-2]\n",
        "        img_dict[\"width\"] = img.shape[-1]\n",
        "        dataset[\"images\"].append(img_dict)\n",
        "        bboxes = targets[\"boxes\"]\n",
        "        bboxes[:, 2:] -= bboxes[:, :2]\n",
        "        bboxes = bboxes.tolist()\n",
        "        labels = targets[\"labels\"].tolist()\n",
        "        areas = targets[\"area\"].tolist()\n",
        "        iscrowd = targets[\"iscrowd\"].tolist()\n",
        "        if \"masks\" in targets:\n",
        "            masks = targets[\"masks\"]\n",
        "            # Make masks Fortran contiguous for coco_mask.\n",
        "            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n",
        "        if \"keypoints\" in targets:\n",
        "            keypoints = targets[\"keypoints\"]\n",
        "            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n",
        "        num_objs = len(bboxes)\n",
        "        for i in range(num_objs):\n",
        "            ann = {}\n",
        "            ann[\"image_id\"] = image_id\n",
        "            ann[\"bbox\"] = bboxes[i]\n",
        "            ann[\"category_id\"] = labels[i]\n",
        "            categories.add(labels[i])\n",
        "            ann[\"area\"] = areas[i]\n",
        "            ann[\"iscrowd\"] = iscrowd[i]\n",
        "            ann[\"id\"] = ann_id\n",
        "            if \"masks\" in targets:\n",
        "                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n",
        "            if \"keypoints\" in targets:\n",
        "                ann[\"keypoints\"] = keypoints[i]\n",
        "                ann[\"num_keypoints\"] = sum(k != 0 for k in keypoints[i][2::3])\n",
        "            dataset[\"annotations\"].append(ann)\n",
        "            ann_id += 1\n",
        "    dataset[\"categories\"] = [{\"id\": i} for i in sorted(categories)]\n",
        "    coco_ds.dataset = dataset\n",
        "    coco_ds.createIndex()\n",
        "    return coco_ds\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "            break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "    return convert_to_coco_api(dataset)\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms):\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super().__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = dict(image_id=image_id, annotations=target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def get_coco(root, image_set, transforms, mode=\"instances\"):\n",
        "    anno_file_template = \"{}_{}2017.json\"\n",
        "    PATHS = {\n",
        "        \"train\": (\"train2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"train\"))),\n",
        "        \"val\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\"))),\n",
        "        # \"train\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\")))\n",
        "    }\n",
        "\n",
        "    t = [ConvertCocoPolysToMask()]\n",
        "\n",
        "    if transforms is not None:\n",
        "        t.append(transforms)\n",
        "    transforms = T.Compose(t)\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    img_folder = os.path.join(root, img_folder)\n",
        "    ann_file = os.path.join(root, ann_file)\n",
        "\n",
        "    dataset = CocoDetection(img_folder, ann_file, transforms=transforms)\n",
        "\n",
        "    if image_set == \"train\":\n",
        "        dataset = _coco_remove_images_without_annotations(dataset)\n",
        "\n",
        "    # dataset = torch.utils.data.Subset(dataset, [i for i in range(500)])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coco_kp(root, image_set, transforms):\n",
        "    return get_coco(root, image_set, transforms, mode=\"person_keypoints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaTkQDRe4yCy"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    coco_evaluator = CocoEvaluator(coco, [\"bbox\"])\n",
        "    model.eval()\n",
        "\n",
        "    for images, targets in data_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # Accumulate predictions from all images.\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    \n",
        "    return coco_evaluator\n",
        "\n",
        "# Evaluation step.\n",
        "# TODO."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "cv_and_dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ed4129e1da34b59a572026c82253b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_745c9a75d9b148f492f176604cc11c80",
              "IPY_MODEL_990d7e70ed7a46b493719c27112c71f6",
              "IPY_MODEL_8caea0806ba9410198b0877e7bb6f2d8"
            ],
            "layout": "IPY_MODEL_3451705cfb9647a2b37ca51c94f1bc4f"
          }
        },
        "745c9a75d9b148f492f176604cc11c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69dee04e330c4288886be1084aad55bd",
            "placeholder": "​",
            "style": "IPY_MODEL_30c928d7a4b745d7895289f60a74bd80",
            "value": "Epoch 0:   0%"
          }
        },
        "990d7e70ed7a46b493719c27112c71f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e322cde81fd4ca4a9dd5a4f7952e238",
            "max": 127,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12908a6020214d0fbbd15c9419cc911b",
            "value": 0
          }
        },
        "8caea0806ba9410198b0877e7bb6f2d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c51285e9e40547d58650bcf647a425fa",
            "placeholder": "​",
            "style": "IPY_MODEL_36ca960bf5d6408787c7f646d33c0f45",
            "value": " 0/127 [00:00&lt;?, ?batch/s]"
          }
        },
        "3451705cfb9647a2b37ca51c94f1bc4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69dee04e330c4288886be1084aad55bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c928d7a4b745d7895289f60a74bd80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e322cde81fd4ca4a9dd5a4f7952e238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12908a6020214d0fbbd15c9419cc911b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c51285e9e40547d58650bcf647a425fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36ca960bf5d6408787c7f646d33c0f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}